"""
ä» arXiv è·å–æœ€æ–°çš„æ— äººæœºå¯¼èˆªç›¸å…³è®ºæ–‡
"""

import feedparser
import urllib.parse
from datetime import datetime, timedelta
from typing import List, Dict
import time
import sys

from config import (
    SEARCH_KEYWORDS,
    NAVIGATION_KEYWORDS,
    PRIORITY_INSTITUTIONS,
    MAX_PAPERS_PER_DAY
)


def build_arxiv_query() -> str:
    """æ„å»º arXiv API æœç´¢æŸ¥è¯¢å­—ç¬¦ä¸²"""
    # æ„å»ºæœç´¢è¯ï¼š(UAV OR drone OR ...) AND (navigation OR path planning OR ...)
    uav_terms = " OR ".join([f'all:"{kw}"' if " " in kw else f'all:{kw}' 
                              for kw in SEARCH_KEYWORDS])
    nav_terms = " OR ".join([f'all:"{kw}"' if " " in kw else f'all:{kw}' 
                              for kw in NAVIGATION_KEYWORDS])
    
    query = f"({uav_terms}) AND ({nav_terms})"
    return query


def fetch_recent_papers(days_back: int = 7, max_results: int = 100) -> List[Dict]:
    """
    ä» arXiv è·å–æœ€è¿‘å‡ å¤©çš„è®ºæ–‡
    
    Args:
        days_back: æŸ¥è¯¢æœ€è¿‘å‡ å¤©çš„è®ºæ–‡
        max_results: æœ€å¤§è¿”å›ç»“æœæ•°
    
    Returns:
        è®ºæ–‡åˆ—è¡¨ï¼Œæ¯ç¯‡è®ºæ–‡åŒ…å« title, authors, summary, link, published, affiliations
    """
    base_url = "http://export.arxiv.org/api/query?"
    
    query = build_arxiv_query()
    
    params = {
        'search_query': query,
        'sortBy': 'submittedDate',
        'sortOrder': 'descending',
        'max_results': max_results,
    }
    
    url = base_url + urllib.parse.urlencode(params)
    
    print(f"ğŸ” æŸ¥è¯¢ arXiv API...")
    print(f"   æŸ¥è¯¢è¯­å¥: {query[:100]}...")
    
    # éµå®ˆ arXiv API ä½¿ç”¨è§„èŒƒï¼šè¯·æ±‚ä¹‹é—´é—´éš” 3 ç§’
    time.sleep(3)
    
    try:
        feed = feedparser.parse(url)
    except Exception as e:
        print(f"âŒ arXiv API è¯·æ±‚å¤±è´¥: {e}")
        return []
    
    if feed.bozo:
        print(f"âš ï¸  è­¦å‘Š: RSS feed è§£æå¯èƒ½æœ‰é—®é¢˜")
    
    papers = []
    cutoff_date = datetime.now() - timedelta(days=days_back)
    
    for entry in feed.entries:
        try:
            # è§£æå‘å¸ƒæ—¥æœŸ
            published = datetime.strptime(entry.published, '%Y-%m-%dT%H:%M:%SZ')
            
            # åªä¿ç•™æœ€è¿‘å‡ å¤©çš„è®ºæ–‡
            if published < cutoff_date:
                continue
            
            # æå–ä½œè€…ä¿¡æ¯
            authors = [author.name for author in entry.authors]
            
            # å°è¯•ä»ä½œè€…ä¿¡æ¯ä¸­æå–æœºæ„ï¼ˆarXiv API å¯èƒ½ä¸ç›´æ¥æä¾›ï¼‰
            # è¿™é‡Œæˆ‘ä»¬å…ˆç®€å•å­˜å‚¨ä½œè€…åˆ—è¡¨ï¼Œåç»­å¯ä»¥æ‰©å±•
            affiliations = []
            
            paper = {
                'title': entry.title.replace('\n', ' ').strip(),
                'authors': authors,
                'summary': entry.summary.replace('\n', ' ').strip(),
                'link': entry.link,
                'pdf_link': entry.link.replace('/abs/', '/pdf/'),
                'published': published.strftime('%Y-%m-%d'),
                'arxiv_id': entry.id.split('/abs/')[-1],
                'affiliations': affiliations,
                'is_priority': False,  # ç¨ååˆ¤æ–­
            }
            
            papers.append(paper)
            
        except Exception as e:
            print(f"âš ï¸  è§£æè®ºæ–‡æ¡ç›®å‡ºé”™: {e}")
            continue
    
    print(f"âœ… è·å–åˆ° {len(papers)} ç¯‡æœ€è¿‘ {days_back} å¤©çš„è®ºæ–‡")
    return papers


def check_priority_institution(paper: Dict) -> bool:
    """
    æ£€æŸ¥è®ºæ–‡æ˜¯å¦æ¥è‡ªä¼˜å…ˆç ”ç©¶æœºæ„
    
    Args:
        paper: è®ºæ–‡å­—å…¸
    
    Returns:
        æ˜¯å¦ä¸ºä¼˜å…ˆæœºæ„
    """
    # åœ¨æ ‡é¢˜ã€ä½œè€…åˆ—è¡¨ã€æ‘˜è¦ä¸­æœç´¢æœºæ„åç§°
    search_text = f"{paper['title']} {' '.join(paper['authors'])} {paper['summary']}".lower()
    
    for institution in PRIORITY_INSTITUTIONS:
        if institution.lower() in search_text:
            return True
    
    return False


def prioritize_papers(papers: List[Dict]) -> List[Dict]:
    """
    ç»™è®ºæ–‡æ’åºï¼Œä¼˜å…ˆæœºæ„çš„è®ºæ–‡æ’åœ¨å‰é¢
    
    Args:
        papers: è®ºæ–‡åˆ—è¡¨
    
    Returns:
        æ’åºåçš„è®ºæ–‡åˆ—è¡¨
    """
    for paper in papers:
        paper['is_priority'] = check_priority_institution(paper)
    
    # æŒ‰ä¼˜å…ˆçº§æ’åºï¼ˆä¼˜å…ˆæœºæ„åœ¨å‰ï¼‰ï¼Œç„¶åæŒ‰æ—¥æœŸ
    sorted_papers = sorted(
        papers,
        key=lambda x: (not x['is_priority'], x['published']),
        reverse=True
    )
    
    priority_count = sum(1 for p in sorted_papers if p['is_priority'])
    print(f"â­ {priority_count} ç¯‡æ¥è‡ªä¼˜å…ˆç ”ç©¶æœºæ„")
    
    return sorted_papers


def filter_papers(papers: List[Dict], max_papers: int = MAX_PAPERS_PER_DAY) -> List[Dict]:
    """
    è¿‡æ»¤å¹¶é™åˆ¶è®ºæ–‡æ•°é‡
    
    Args:
        papers: è®ºæ–‡åˆ—è¡¨
        max_papers: æœ€å¤§ä¿ç•™è®ºæ–‡æ•°
    
    Returns:
        è¿‡æ»¤åçš„è®ºæ–‡åˆ—è¡¨
    """
    # å…ˆæ’åº
    papers = prioritize_papers(papers)
    
    # é™åˆ¶æ•°é‡
    if len(papers) > max_papers:
        print(f"ğŸ“Š é™åˆ¶è®ºæ–‡æ•°é‡: {len(papers)} -> {max_papers}")
        papers = papers[:max_papers]
    
    return papers


def main():
    """ä¸»å‡½æ•° - è·å–å¹¶è¿‡æ»¤è®ºæ–‡"""
    print("=" * 60)
    print("ğŸ“š å¼€å§‹è·å– arXiv æ— äººæœºå¯¼èˆªè®ºæ–‡")
    print("=" * 60)
    
    # è·å–æœ€è¿‘ 3 å¤©çš„è®ºæ–‡ï¼ˆé¿å…é—æ¼å‘¨æœ«æ›´æ–°ï¼‰
    papers = fetch_recent_papers(days_back=3, max_results=100)
    
    if not papers:
        print("âŒ æ²¡æœ‰æ‰¾åˆ°ç›¸å…³è®ºæ–‡")
        return []
    
    # è¿‡æ»¤å’Œæ’åº
    filtered_papers = filter_papers(papers)
    
    print("\n" + "=" * 60)
    print(f"âœ… æœ€ç»ˆç­›é€‰å‡º {len(filtered_papers)} ç¯‡è®ºæ–‡å¾…åˆ†æ")
    print("=" * 60)
    
    # æ˜¾ç¤ºå‰å‡ ç¯‡è®ºæ–‡æ ‡é¢˜
    print("\nğŸ“‘ è®ºæ–‡é¢„è§ˆ:")
    for i, paper in enumerate(filtered_papers[:5], 1):
        priority_mark = "â­" if paper['is_priority'] else "  "
        print(f"{priority_mark} {i}. {paper['title'][:80]}...")
    
    if len(filtered_papers) > 5:
        print(f"   ... è¿˜æœ‰ {len(filtered_papers) - 5} ç¯‡è®ºæ–‡")
    
    return filtered_papers


if __name__ == "__main__":
    papers = main()
    
    # è¿”å›è®ºæ–‡æ•°é‡ä½œä¸ºé€€å‡ºç ï¼ˆç”¨äºè°ƒè¯•ï¼‰
    sys.exit(0 if papers else 1)
